{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n",
      "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"\n",
      "INFO:neo4j.notifications:Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE CONSTRAINT IF NOT EXISTS FOR (e:__Node__) REQUIRE (e.id) IS UNIQUE` has no effect.} {description: `CONSTRAINT constraint_ec67c859 FOR (e:__Node__) REQUIRE (e.id) IS UNIQUE` already exists.} {position: None} for query: 'CREATE CONSTRAINT IF NOT EXISTS FOR (n:`__Node__`)\\n            REQUIRE n.id IS UNIQUE;'\n",
      "Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE CONSTRAINT IF NOT EXISTS FOR (e:__Node__) REQUIRE (e.id) IS UNIQUE` has no effect.} {description: `CONSTRAINT constraint_ec67c859 FOR (e:__Node__) REQUIRE (e.id) IS UNIQUE` already exists.} {position: None} for query: 'CREATE CONSTRAINT IF NOT EXISTS FOR (n:`__Node__`)\\n            REQUIRE n.id IS UNIQUE;'\n",
      "Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE CONSTRAINT IF NOT EXISTS FOR (e:__Node__) REQUIRE (e.id) IS UNIQUE` has no effect.} {description: `CONSTRAINT constraint_ec67c859 FOR (e:__Node__) REQUIRE (e.id) IS UNIQUE` already exists.} {position: None} for query: 'CREATE CONSTRAINT IF NOT EXISTS FOR (n:`__Node__`)\\n            REQUIRE n.id IS UNIQUE;'\n",
      "INFO:neo4j.notifications:Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE CONSTRAINT IF NOT EXISTS FOR (e:__Entity__) REQUIRE (e.id) IS UNIQUE` has no effect.} {description: `CONSTRAINT constraint_907a464e FOR (e:__Entity__) REQUIRE (e.id) IS UNIQUE` already exists.} {position: None} for query: 'CREATE CONSTRAINT IF NOT EXISTS FOR (n:`__Entity__`)\\n            REQUIRE n.id IS UNIQUE;'\n",
      "Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE CONSTRAINT IF NOT EXISTS FOR (e:__Entity__) REQUIRE (e.id) IS UNIQUE` has no effect.} {description: `CONSTRAINT constraint_907a464e FOR (e:__Entity__) REQUIRE (e.id) IS UNIQUE` already exists.} {position: None} for query: 'CREATE CONSTRAINT IF NOT EXISTS FOR (n:`__Entity__`)\\n            REQUIRE n.id IS UNIQUE;'\n",
      "Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE CONSTRAINT IF NOT EXISTS FOR (e:__Entity__) REQUIRE (e.id) IS UNIQUE` has no effect.} {description: `CONSTRAINT constraint_907a464e FOR (e:__Entity__) REQUIRE (e.id) IS UNIQUE` already exists.} {position: None} for query: 'CREATE CONSTRAINT IF NOT EXISTS FOR (n:`__Entity__`)\\n            REQUIRE n.id IS UNIQUE;'\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "from llama_index.core import Settings, ServiceContext, StorageContext, SimpleDirectoryReader, PropertyGraphIndex\n",
    "from llama_index.llms.groq import Groq as Groq_llamaindex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.replicate import Replicate\n",
    "#from llama_index.llms.ollama import Ollama as Ollama_llamaindex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.graph_stores.neo4j import Neo4jGraphStore, Neo4jPropertyGraphStore\n",
    "#from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.agent import ReActAgent, FunctionCallingAgentWorker, AgentRunner\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "from milvus import default_server\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "# Retrieve API keys and credentials securely\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "OPEN_AI_API_KEY = os.getenv('OPEN_AI_API_KEY')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_URL = os.getenv('NEO4J_URL', 'bolt://localhost:7687')\n",
    "NEO4J_DATABASE = os.getenv('NEO4J_DATABASE', 'neo4j')\n",
    "REPLICATE_API_KEY = os.getenv('REPLICATE_API_KEY')\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_KEY\n",
    "\n",
    "\n",
    "#Initialize the Replicate class\n",
    "llm = Replicate(\n",
    "    model=\"meta/meta-llama-3-70b-instruct\",\n",
    "    api_key=REPLICATE_API_KEY,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "\n",
    "ServiceContext.llm = llm\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "username = NEO4J_USERNAME\n",
    "password = NEO4J_PASSWORD\n",
    "url = NEO4J_URL\n",
    "database = NEO4J_DATABASE\n",
    "\n",
    "property_graph_store = Neo4jPropertyGraphStore(\n",
    "    username=username,\n",
    "    password=password,\n",
    "    url=url,\n",
    "    database=database,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(property_graph_store=property_graph_store)\n",
    "\n",
    "\n",
    "index = PropertyGraphIndex.from_existing(\n",
    "    property_graph_store=property_graph_store,\n",
    "    llm=llm,\n",
    "    embed_model=Settings.embed_model,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-70b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-70b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-70b-instruct/predictions \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text=\"\\n\\nHi! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, raw=None, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Initialize the Replicate class\n",
    "llm = Replicate(\n",
    "    model=\"meta/meta-llama-3-70b-instruct\"\n",
    "    #model=\"meta/meta-llama-3.1-405b-instruct\"\n",
    ")#l\n",
    "\n",
    "llm.complete('hi')\n",
    "#llmr.complete('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.indices.property_graph import VectorContextRetriever\n",
    "similarity_top_k = 4    \n",
    "path_depth = 3\n",
    "vector_retriever = VectorContextRetriever(\n",
    "    index.property_graph_store,\n",
    "    # only needed when the graph store doesn't support vector queries\n",
    "    # vector_store=index.vector_store,\n",
    "    embed_model=Settings.embed_model,\n",
    "    # include source chunk text with retrieved paths\n",
    "    include_text=True,\n",
    "    # the number of nodes to fetch\n",
    "    similarity_top_k=similarity_top_k,\n",
    "    # the depth of relations to follow after node retrieval\n",
    "    path_depth=path_depth,\n",
    "    \n",
    "  \n",
    ")\n",
    "\n",
    "#retriever = index.as_retriever(sub_retrievers=[vector_retriever])\n",
    "\n",
    "index_query_engine = index.as_query_engine(sub_retrievers=[vector_retriever])\n",
    "index_retriever = index.as_retriever(sub_retrievers=[vector_retriever])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Apply asyncio patch for Jupyter notebooks\n",
    "# Read questions from file selected_questions_set_2.csv\n",
    "questions = pd.read_csv(\"oia_files_and_questions.csv\")[\"Question\"].tolist()\n",
    "# Load the already completed answers from temp_answers.csv\n",
    "# try to rad temp_answers.csv if it does not exist create a blank df with column 'Answer'\n",
    "try:\n",
    "    df_answers = pd.read_csv(\"temp_answers.csv\")\n",
    "except FileNotFoundError:\n",
    "    df_answers = pd.DataFrame(columns=[\"Question\", \"Answer\"])\n",
    "\n",
    "# Ensure the answers list matches the length of the questions list\n",
    "answers = df_answers[\"Answer\"].tolist() + [None] * (len(questions) - len(df_answers))\n",
    "\n",
    "# Loop over all questions where the answer is None or 'none'\n",
    "for idx, (question, answer) in enumerate(zip(questions, answers)):\n",
    "    if answer is None or answer.strip().lower() == 'none':\n",
    "        print(f\"Processing question {idx}\")\n",
    "        print(question)\n",
    "\n",
    "        try:\n",
    "            response = index_query_engine.query(question)\n",
    "\n",
    "            answers[idx] = response.response\n",
    "\n",
    "            # Create a new row as a DataFrame\n",
    "            new_row = pd.DataFrame({'Question': [question], 'Answer': [response.response]})\n",
    "            # Concatenate the new row to the existing DataFrame\n",
    "            df_answers = pd.concat([df_answers, new_row], ignore_index=True)\n",
    "            # Save the updated dataframe to 'temp_answers.csv'\n",
    "            df_answers.to_csv(\"temp_answers.csv\", index=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            answers[idx] = None\n",
    "            print(f\"Error: {e}\")\n",
    "            \n",
    "        print(\"\\n#######################\\n\")\n",
    "        time.sleep(0)  # Pause for 20 seconds\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chunk bellow is not used any more other that to format the answer the way we need them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the questions and answers DataFrames\n",
    "questions = pd.read_csv(\"oia_files_and_questions.csv\")  # Contains correct answers in 'Answer' column\n",
    "answers = pd.read_csv(\"temp_answers.csv\")  # Contains model-generated answers\n",
    "\n",
    "# Ensure that the model-generated answers are in a column named 'Model Answer'\n",
    "if 'Answer' in answers.columns:\n",
    "    answers.rename(columns={'Answer': 'Model Answer'}, inplace=True)\n",
    "\n",
    "# Add the 'Model Answer' from the answers DataFrame to the questions DataFrame\n",
    "questions[\"Model Answer\"] = answers[\"Model Answer\"]\n",
    "\n",
    "# Now, we will compare the correct answers in questions['Answer'] with the model's answers in questions['Model Answer']\n",
    "evaluation_results = []\n",
    "\n",
    "for idx, row in questions.iterrows():\n",
    "    # Create a comparison prompt\n",
    "    prompt = f\"\"\"\n",
    "    The question is: {row['Question']}\n",
    "    The correct answer is: {row['Answer']}\n",
    "    The model's answer is: {row['Model Answer']}\n",
    "    \n",
    "    Based on the model's answer, respond with one of the following:\n",
    "    - correct\n",
    "    - wrong\n",
    "    - did not find\n",
    "\n",
    "    do not use any other words in your response\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call the language model to complete the prompt\n",
    "    evaluation = \"correct\"\n",
    "\n",
    "    # Append the result to the evaluation_results list\n",
    "    #evaluation_results.append(evaluation.text.strip())  # Strip any whitespace\n",
    "    evaluation_results.append(evaluation)\n",
    "\n",
    "    # Print or store the evaluation result\n",
    "    #print(f\"Question {idx+1}: {evaluation.text.strip()}\")\n",
    "\n",
    "# Optionally, add the evaluation results as a new column in the questions DataFrame\n",
    "questions[\"Evaluation\"] = evaluation_results\n",
    "\n",
    "# Save the updated DataFrame with the evaluation results\n",
    "questions.to_csv(\"evaluated_answers.csv\", index=False)\n",
    "\n",
    "\n",
    "# make directoyr if it does not exist\n",
    "Path(\"results_temp\").mkdir(parents=True, exist_ok=True)\n",
    "questions.to_csv(f\"results_temp/evaluated_unsorted_k{similarity_top_k}_d{path_depth}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting datasets>=2.0.0 (from evaluate)\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./nzta_experiment_env/lib/python3.11/site-packages (from evaluate) (1.26.4)\n",
      "Collecting dill (from evaluate)\n",
      "  Using cached dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in ./nzta_experiment_env/lib/python3.11/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./nzta_experiment_env/lib/python3.11/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./nzta_experiment_env/lib/python3.11/site-packages (from evaluate) (4.66.5)\n",
      "Collecting xxhash (from evaluate)\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Using cached multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in ./nzta_experiment_env/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in ./nzta_experiment_env/lib/python3.11/site-packages (from evaluate) (0.25.1)\n",
      "Requirement already satisfied: packaging in ./nzta_experiment_env/lib/python3.11/site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in ./nzta_experiment_env/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.0.0->evaluate)\n",
      "  Downloading pyarrow-18.0.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting dill (from evaluate)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: aiohttp in ./nzta_experiment_env/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.10.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./nzta_experiment_env/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./nzta_experiment_env/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./nzta_experiment_env/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./nzta_experiment_env/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./nzta_experiment_env/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./nzta_experiment_env/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./nzta_experiment_env/lib/python3.11/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./nzta_experiment_env/lib/python3.11/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./nzta_experiment_env/lib/python3.11/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./nzta_experiment_env/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./nzta_experiment_env/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./nzta_experiment_env/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./nzta_experiment_env/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./nzta_experiment_env/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./nzta_experiment_env/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.12.1)\n",
      "Requirement already satisfied: six>=1.5 in ./nzta_experiment_env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Using cached evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading pyarrow-18.0.0-cp311-cp311-macosx_12_0_arm64.whl (29.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.5/29.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets, evaluate\n",
      "Successfully installed datasets-3.1.0 dill-0.3.8 evaluate-0.4.3 multiprocess-0.70.16 pyarrow-18.0.0 xxhash-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk rouge-score datasets\n",
    "!pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/felipenavarro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/felipenavarro/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# Ensure that NLTK has the necessary package for BLEU\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the BGE model and tokenizer\n",
    "MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "bge_model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "bge_model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Function to generate embeddings using BGE with caching\n",
    "class EmbeddingCache:\n",
    "    def __init__(self, tokenizer, model, device='cpu', batch_size=32):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.cache = {}\n",
    "    \n",
    "    def get_embeddings(self, texts):\n",
    "        \"\"\"\n",
    "        Generates embeddings for a list of texts with caching to avoid redundant computations.\n",
    "        \n",
    "        Args:\n",
    "            texts (List[str]): List of text strings to embed.\n",
    "        \n",
    "        Returns:\n",
    "            List[np.ndarray]: List of embedding vectors.\n",
    "        \"\"\"\n",
    "        texts_to_embed = [text for text in texts if text not in self.cache]\n",
    "        if texts_to_embed:\n",
    "            for i in tqdm(range(0, len(texts_to_embed), self.batch_size), desc=\"Embedding texts\"):\n",
    "                batch = texts_to_embed[i:i+self.batch_size]\n",
    "                inputs = self.tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    embeddings = self.model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "                for text, emb in zip(batch, embeddings):\n",
    "                    self.cache[text] = emb\n",
    "        return [self.cache[text] for text in texts]\n",
    "\n",
    "embedding_cache = EmbeddingCache(tokenizer, bge_model, device='cpu', batch_size=32)\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def calculate_cosine_similarity(embeddings1, embeddings2):\n",
    "    return cosine_similarity(embeddings1, embeddings2).diagonal()\n",
    "\n",
    "# BLEU score calculation\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    reference = [nltk.word_tokenize(reference)]  # Reference must be a list of lists for BLEU\n",
    "    hypothesis = nltk.word_tokenize(hypothesis)\n",
    "    return sentence_bleu(reference, hypothesis)\n",
    "\n",
    "# ROUGE score calculation\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def calculate_rouge_per_row(reference, prediction):\n",
    "    \"\"\"\n",
    "    Compute ROUGE scores for a single (reference, prediction) pair.\n",
    "    \n",
    "    Args:\n",
    "        reference (str): The reference text.\n",
    "        prediction (str): The prediction text.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing ROUGE-1, ROUGE-2, and ROUGE-L scores.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        scores = rouge.compute(predictions=[prediction], references=[reference])\n",
    "        return {\n",
    "            'ROUGE-1': scores['rouge1'],\n",
    "            'ROUGE-2': scores['rouge2'],\n",
    "            'ROUGE-L': scores['rougeL']\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing ROUGE for reference: {reference} and prediction: {prediction}\\n{e}\")\n",
    "        return {'ROUGE-1': float('nan'), 'ROUGE-2': float('nan'), 'ROUGE-L': float('nan')}\n",
    "\n",
    "# Directory containing the CSV files\n",
    "DIRECTORY = 'results_temp'\n",
    "\n",
    "# Regex pattern to parse filenames\n",
    "FILENAME_PATTERN = re.compile(\n",
    "    r'evaluated_(?P<sort_status>sorted|unsorted)_k(?P<k>\\d+)_d(?P<d>\\d+)\\.csv$'\n",
    ")\n",
    "\n",
    "# List to collect all records\n",
    "records = []\n",
    "\n",
    "# Iterate through all CSV files in the directory with a progress bar\n",
    "csv_files = [f for f in os.listdir(DIRECTORY) if f.endswith(\".csv\")]\n",
    "for filename in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "    match = FILENAME_PATTERN.match(filename)\n",
    "    if not match:\n",
    "        print(f\"Filename format not recognized: {filename}\")\n",
    "        continue\n",
    "    \n",
    "    sort_status = match.group('sort_status')\n",
    "    k_value = int(match.group('k'))\n",
    "    d_value = int(match.group('d'))\n",
    "    \n",
    "    file_path = os.path.join(DIRECTORY, filename)\n",
    "    \n",
    "    # Load the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Ensure all entries in 'Answer' and 'Model Answer' are strings\n",
    "    data['Answer'] = data['Answer'].astype(str)\n",
    "    data['Model Answer'] = data['Model Answer'].astype(str)\n",
    "    \n",
    "    # Collect all unique references and hypotheses for embedding\n",
    "    unique_texts = pd.concat([data['Answer'], data['Model Answer']]).unique()\n",
    "    embedding_cache.get_embeddings(unique_texts)\n",
    "    \n",
    "    # Calculate embeddings\n",
    "    data['Reference Embedding'] = embedding_cache.get_embeddings(data['Answer'])\n",
    "    data['Hypothesis Embedding'] = embedding_cache.get_embeddings(data['Model Answer'])\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    data['Cosine Similarity'] = calculate_cosine_similarity(\n",
    "        list(data['Reference Embedding']),\n",
    "        list(data['Hypothesis Embedding'])\n",
    "    )\n",
    "    \n",
    "    # Calculate BLEU scores with a progress bar\n",
    "    bleu_scores = []\n",
    "    for idx, row in tqdm(data.iterrows(), total=data.shape[0], desc=f\"Calculating BLEU for {filename}\", leave=False):\n",
    "        reference = row['Answer']\n",
    "        hypothesis = row['Model Answer']\n",
    "        bleu = calculate_bleu(reference, hypothesis)\n",
    "        bleu_scores.append(bleu)\n",
    "    data['BLEU'] = bleu_scores\n",
    "    \n",
    "    # Calculate ROUGE scores per row with a progress bar\n",
    "    rouge_scores_1 = []\n",
    "    rouge_scores_2 = []\n",
    "    rouge_scores_L = []\n",
    "    for idx, row in tqdm(data.iterrows(), total=data.shape[0], desc=f\"Calculating ROUGE for {filename}\", leave=False):\n",
    "        reference = row['Answer']\n",
    "        prediction = row['Model Answer']\n",
    "        rouge_result = calculate_rouge_per_row(reference, prediction)\n",
    "        rouge_scores_1.append(rouge_result['ROUGE-1'])\n",
    "        rouge_scores_2.append(rouge_result['ROUGE-2'])\n",
    "        rouge_scores_L.append(rouge_result['ROUGE-L'])\n",
    "    data['ROUGE-1'] = rouge_scores_1\n",
    "    data['ROUGE-2'] = rouge_scores_2\n",
    "    data['ROUGE-L'] = rouge_scores_L\n",
    "    \n",
    "    # Add metadata\n",
    "    data['Sort Status'] = sort_status\n",
    "    data['k'] = k_value\n",
    "    data['d'] = d_value\n",
    "    \n",
    "    # Append relevant columns to records\n",
    "    records.append(data[['Question', 'Sort Status', 'k', 'd', 'BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'Cosine Similarity']])\n",
    "\n",
    "# Concatenate all records into a single DataFrame\n",
    "summary_results = pd.concat(records, ignore_index=True)\n",
    "\n",
    "# Save the summary DataFrame to a CSV file\n",
    "summary_csv_path = os.path.join(DIRECTORY, 'summary_evaluations.csv')\n",
    "summary_results.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "# Optionally, print the first few rows of the summary results\n",
    "print(summary_results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully.\n",
      "Unique k values found: [4]\n",
      "\n",
      "Processing k=4...\n",
      "Aggregating metrics...\n",
      "Aggregation complete.\n",
      "\n",
      "Aggregated Data:\n",
      "   d Sort Status      BLEU   ROUGE-1   ROUGE-2  ROUGE-L  Cosine Similarity\n",
      "0  3    unsorted  0.057199  0.237928  0.121293  0.19396           0.721254\n",
      "\n",
      "Creating plots...\n",
      "Saved plot: plots_k4/bleu_comparison_k4.png\n",
      "Saved plot: plots_k4/rouge-1_comparison_k4.png\n",
      "Saved plot: plots_k4/rouge-2_comparison_k4.png\n",
      "Saved plot: plots_k4/rouge-l_comparison_k4.png\n",
      "Saved plot: plots_k4/cosine_similarity_comparison_k4.png\n",
      "All plots for k=4 created and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# evaluation_plots.py  This is the working version that does the plots\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def load_data(summary_csv_path):\n",
    "    \"\"\"\n",
    "    Load the summary evaluations CSV into a Pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        summary_csv_path (str): Path to the summary_evaluations.csv file.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded DataFrame.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(summary_csv_path):\n",
    "        raise FileNotFoundError(f\"File not found: {summary_csv_path}\")\n",
    "    \n",
    "    df = pd.read_csv(summary_csv_path)\n",
    "    return df\n",
    "\n",
    "def filter_data_by_k(df, k_value):\n",
    "    \"\"\"\n",
    "    Filter the DataFrame for a specific value of k.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "        k_value (int): The k value to filter by.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame.\n",
    "    \"\"\"\n",
    "    filtered_df = df[df['k'] == k_value]\n",
    "    if filtered_df.empty:\n",
    "        print(f\"No data found for k={k_value}. Skipping.\")\n",
    "    return filtered_df\n",
    "\n",
    "def aggregate_metrics(df, metrics, group_by=['d', 'Sort Status']):\n",
    "    \"\"\"\n",
    "    Aggregate the DataFrame by computing the mean of specified metrics,\n",
    "    grouped by the specified columns.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to aggregate.\n",
    "        metrics (list): List of metric column names to aggregate.\n",
    "        group_by (list): Columns to group by.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Aggregated DataFrame.\n",
    "    \"\"\"\n",
    "    aggregated_df = df.groupby(group_by)[metrics].mean().reset_index()\n",
    "    return aggregated_df\n",
    "\n",
    "def create_plots(aggregated_df, metrics, k_value, output_dir='plots'):\n",
    "    \"\"\"\n",
    "    Create and save comparative plots for each metric for a specific k value.\n",
    "    \n",
    "    Args:\n",
    "        aggregated_df (pd.DataFrame): Aggregated DataFrame with mean scores.\n",
    "        metrics (list): List of metric column names to plot.\n",
    "        k_value (int): The k value being processed.\n",
    "        output_dir (str): Directory to save the plots.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Set the aesthetic style of the plots\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    \n",
    "    for metric in metrics:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Create a line plot with d on x-axis and metric score on y-axis\n",
    "        sns.lineplot(\n",
    "            data=aggregated_df,\n",
    "            x='d',\n",
    "            y=metric,\n",
    "            hue='Sort Status',\n",
    "            marker='o',\n",
    "            palette='viridis'\n",
    "        )\n",
    "        \n",
    "        # Set plot titles and labels\n",
    "        plt.title(f'{metric} Comparison for k={k_value}: Sorted vs Unsorted')\n",
    "        plt.xlabel('d Value')\n",
    "        plt.ylabel(metric)\n",
    "        \n",
    "        # Enhance legend\n",
    "        plt.legend(title='Sort Status')\n",
    "        \n",
    "        # Optionally, add annotations for exact values\n",
    "        for line in plt.gca().get_lines():\n",
    "            x_values = line.get_xdata()\n",
    "            y_values = line.get_ydata()\n",
    "            for x, y in zip(x_values, y_values):\n",
    "                plt.text(x, y, f'{y:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_filename = f\"{metric.replace(' ', '_').lower()}_comparison_k{k_value}.png\"\n",
    "        plt.savefig(os.path.join(output_dir, plot_filename), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved plot: {os.path.join(output_dir, plot_filename)}\")\n",
    "\n",
    "\n",
    "# Path to the summary evaluations CSV\n",
    "summary_csv_path = 'results_temp/summary_evaluations.csv'\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "df = load_data(summary_csv_path)\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# Get the unique k values in the data\n",
    "k_values = df['k'].unique()\n",
    "print(f\"Unique k values found: {k_values}\")\n",
    "\n",
    "# Define the metrics to plot\n",
    "metrics = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'Cosine Similarity']\n",
    "\n",
    "# Check if all metrics exist in the DataFrame\n",
    "missing_metrics = [metric for metric in metrics if metric not in df.columns]\n",
    "if missing_metrics:\n",
    "    raise ValueError(f\"The following metrics are missing in the data: {missing_metrics}\")\n",
    "\n",
    "# Loop over each k value\n",
    "for k_value in k_values:\n",
    "    print(f\"\\nProcessing k={k_value}...\")\n",
    "    \n",
    "    # Filter the DataFrame for the current k value\n",
    "    df_k = filter_data_by_k(df, k_value)\n",
    "    if df_k.empty:\n",
    "        continue  # Skip if no data for this k\n",
    "    \n",
    "    # Aggregate the data by computing mean scores for each metric\n",
    "    print(\"Aggregating metrics...\")\n",
    "    aggregated_df = aggregate_metrics(df_k, metrics)\n",
    "    print(\"Aggregation complete.\")\n",
    "    \n",
    "    # Display the aggregated data (optional)\n",
    "    print(\"\\nAggregated Data:\")\n",
    "    print(aggregated_df.head())\n",
    "    \n",
    "    # Create and save the plots\n",
    "    print(\"\\nCreating plots...\")\n",
    "    output_dir_k = f'plots_k{k_value}'\n",
    "    create_plots(aggregated_df, metrics, k_value, output_dir=output_dir_k)\n",
    "    print(f\"All plots for k={k_value} created and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully.\n",
      "\n",
      "Analyzing BLEU for k=4, d=3...\n",
      "No paired data available for BLEU with k=4, d=3. Skipping...\n",
      "\n",
      "Analyzing ROUGE-1 for k=4, d=3...\n",
      "No paired data available for ROUGE-1 with k=4, d=3. Skipping...\n",
      "\n",
      "Analyzing ROUGE-2 for k=4, d=3...\n",
      "No paired data available for ROUGE-2 with k=4, d=3. Skipping...\n",
      "\n",
      "Analyzing ROUGE-L for k=4, d=3...\n",
      "No paired data available for ROUGE-L with k=4, d=3. Skipping...\n",
      "\n",
      "Analyzing Cosine Similarity for k=4, d=3...\n",
      "No paired data available for Cosine Similarity with k=4, d=3. Skipping...\n",
      "\n",
      "Differences DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics of Differences:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot describe a DataFrame without columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 284\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# Optionally, display summary statistics\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSummary Statistics of Differences:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 284\u001b[0m display(\u001b[43mdifferences_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# 9. Summarize and Save Statistical Test Results\u001b[39;00m\n\u001b[1;32m    287\u001b[0m summarize_results(test_results)\n",
      "File \u001b[0;32m~/Documents/Auckland/nzta/nzta_experiment/NZTA-GraphRAG-Experiment/nzta_experiment_env/lib/python3.11/site-packages/pandas/core/generic.py:11976\u001b[0m, in \u001b[0;36mNDFrame.describe\u001b[0;34m(self, percentiles, include, exclude)\u001b[0m\n\u001b[1;32m  11734\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m  11735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdescribe\u001b[39m(\n\u001b[1;32m  11736\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11739\u001b[0m     exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m  11740\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m  11741\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m  11742\u001b[0m \u001b[38;5;124;03m    Generate descriptive statistics.\u001b[39;00m\n\u001b[1;32m  11743\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11974\u001b[0m \u001b[38;5;124;03m    max            NaN      3.0\u001b[39;00m\n\u001b[1;32m  11975\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m> 11976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdescribe_ndframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  11977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m  11978\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  11979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  11980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpercentiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpercentiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  11981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescribe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Auckland/nzta/nzta_experiment/NZTA-GraphRAG-Experiment/nzta_experiment_env/lib/python3.11/site-packages/pandas/core/methods/describe.py:91\u001b[0m, in \u001b[0;36mdescribe_ndframe\u001b[0;34m(obj, include, exclude, percentiles)\u001b[0m\n\u001b[1;32m     87\u001b[0m     describer \u001b[38;5;241m=\u001b[39m SeriesDescriber(\n\u001b[1;32m     88\u001b[0m         obj\u001b[38;5;241m=\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries\u001b[39m\u001b[38;5;124m\"\u001b[39m, obj),\n\u001b[1;32m     89\u001b[0m     )\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     describer \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrameDescriber\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDataFrame\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m result \u001b[38;5;241m=\u001b[39m describer\u001b[38;5;241m.\u001b[39mdescribe(percentiles\u001b[38;5;241m=\u001b[39mpercentiles)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(NDFrameT, result)\n",
      "File \u001b[0;32m~/Documents/Auckland/nzta/nzta_experiment/NZTA-GraphRAG-Experiment/nzta_experiment_env/lib/python3.11/site-packages/pandas/core/methods/describe.py:162\u001b[0m, in \u001b[0;36mDataFrameDescriber.__init__\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexclude \u001b[38;5;241m=\u001b[39m exclude\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot describe a DataFrame without columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(obj)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot describe a DataFrame without columns"
     ]
    }
   ],
   "source": [
    "# statistical_tests_updated_single_block_with_k.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro, ttest_rel, wilcoxon, binomtest\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "\n",
    "# 1. Load Data\n",
    "def load_data(summary_csv_path):\n",
    "    \"\"\"\n",
    "    Load the summary evaluations CSV into a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        summary_csv_path (str): Path to the summary_evaluations.csv file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded DataFrame.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(summary_csv_path):\n",
    "        raise FileNotFoundError(f\"File not found: {summary_csv_path}\")\n",
    "\n",
    "    df = pd.read_csv(summary_csv_path)\n",
    "    return df\n",
    "\n",
    "# 2. Prepare Paired Data\n",
    "def prepare_paired_data(df, metric, d_value, k_value):\n",
    "    \"\"\"\n",
    "    Prepare paired data for a specific metric, d value, and k value.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "        metric (str): The metric to analyze.\n",
    "        d_value (int): The d value to filter by.\n",
    "        k_value (int): The k value to filter by.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame with sorted and unsorted scores.\n",
    "    \"\"\"\n",
    "    filtered_df = df[(df['d'] == d_value) & (df['k'] == k_value)]\n",
    "    sorted_scores = filtered_df[filtered_df['Sort Status'] == 'sorted'][metric].reset_index(drop=True)\n",
    "    unsorted_scores = filtered_df[filtered_df['Sort Status'] == 'unsorted'][metric].reset_index(drop=True)\n",
    "\n",
    "    # Ensure both sorted and unsorted have the same number of entries\n",
    "    min_length = min(len(sorted_scores), len(unsorted_scores))\n",
    "    if min_length == 0:\n",
    "        return pd.DataFrame()  # Return empty DataFrame if no paired data\n",
    "\n",
    "    sorted_scores = sorted_scores[:min_length]\n",
    "    unsorted_scores = unsorted_scores[:min_length]\n",
    "\n",
    "    paired_df = pd.DataFrame({\n",
    "        'sorted': sorted_scores,\n",
    "        'unsorted': unsorted_scores\n",
    "    })\n",
    "\n",
    "    return paired_df\n",
    "\n",
    "# 3. Perform Normality Test\n",
    "def perform_normality_test(differences):\n",
    "    \"\"\"\n",
    "    Perform Shapiro-Wilk test for normality on the differences.\n",
    "\n",
    "    Args:\n",
    "        differences (pd.Series): Differences between sorted and unsorted scores.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (statistic, p-value)\n",
    "    \"\"\"\n",
    "    if len(differences) < 3:\n",
    "        # Shapiro-Wilk test requires at least 3 data points\n",
    "        return np.nan, np.nan\n",
    "    stat, p = shapiro(differences)\n",
    "    return stat, p\n",
    "\n",
    "# 4. Perform Statistical Tests\n",
    "def perform_statistical_tests(paired_df):\n",
    "    \"\"\"\n",
    "    Perform paired t-test or Wilcoxon signed-rank test based on normality.\n",
    "    Also perform the sign test.\n",
    "\n",
    "    Args:\n",
    "        paired_df (pd.DataFrame): DataFrame with 'sorted' and 'unsorted' columns.\n",
    "\n",
    "    Returns:\n",
    "        dict: Test results including test names, statistics, and p-values.\n",
    "    \"\"\"\n",
    "    differences = paired_df['sorted'] - paired_df['unsorted']\n",
    "    stat, p = perform_normality_test(differences)\n",
    "\n",
    "    normality = False\n",
    "    if not np.isnan(p):\n",
    "        normality = p > 0.05  # If p > 0.05, assume normality\n",
    "\n",
    "    # Initialize variables\n",
    "    t_stat = np.nan\n",
    "    t_p = np.nan\n",
    "    w_stat = np.nan\n",
    "    w_p = np.nan\n",
    "    s_stat = np.nan\n",
    "    s_p = np.nan\n",
    "\n",
    "    # Paired t-test if normality holds\n",
    "    if normality:\n",
    "        if len(paired_df) >= 2:\n",
    "            t_stat, t_p = ttest_rel(paired_df['sorted'], paired_df['unsorted'])\n",
    "        else:\n",
    "            print(\"Paired t-test not performed due to insufficient data.\")\n",
    "\n",
    "    # Wilcoxon signed-rank test\n",
    "    if len(paired_df) >= 1:\n",
    "        try:\n",
    "            w_stat, w_p = wilcoxon(paired_df['sorted'], paired_df['unsorted'])\n",
    "        except ValueError as e:\n",
    "            print(f\"Wilcoxon test not performed: {e}\")\n",
    "    else:\n",
    "        print(\"Wilcoxon test not performed due to insufficient data.\")\n",
    "\n",
    "    # Sign test\n",
    "    non_zero_differences = differences[differences != 0]\n",
    "    n = len(non_zero_differences)\n",
    "    if n == 0:\n",
    "        print(\"Sign test not performed due to all differences being zero.\")\n",
    "    else:\n",
    "        n_positive = sum(non_zero_differences > 0)\n",
    "        # Under H0, the probability of positive difference is 0.5\n",
    "        # Perform a binomial test\n",
    "        binom_result = binomtest(n_positive, n, 0.5, alternative='two-sided')\n",
    "        s_stat = n_positive\n",
    "        s_p = binom_result.pvalue\n",
    "\n",
    "    # Return results including normality test results\n",
    "    final_results = {\n",
    "        'Normality': normality,\n",
    "        'Shapiro-Wilk Statistic': stat,\n",
    "        'Shapiro-Wilk p-value': p,\n",
    "        'Paired t-test Statistic': t_stat,\n",
    "        'Paired t-test p-value': t_p,\n",
    "        'Wilcoxon test Statistic': w_stat,\n",
    "        'Wilcoxon test p-value': w_p,\n",
    "        'Sign test Statistic': s_stat,\n",
    "        'Sign test p-value': s_p\n",
    "    }\n",
    "    return final_results\n",
    "\n",
    "# 5. Plot All Histograms\n",
    "def plot_all_histograms(differences_dict, metrics, d_values, k_values, output_dir='histograms'):\n",
    "    \"\"\"\n",
    "    Plot histograms of differences for each metric, d, and k value.\n",
    "\n",
    "    Args:\n",
    "        differences_dict (dict): Nested dictionary containing differences.\n",
    "        metrics (list): List of metrics.\n",
    "        d_values (list): List of d values.\n",
    "        k_values (list): List of k values.\n",
    "        output_dir (str): Directory to save the plots.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for metric in metrics:\n",
    "        for k in k_values:\n",
    "            # Prepare subplots\n",
    "            num_d = len(d_values)\n",
    "            cols = 3\n",
    "            rows = ceil(num_d / cols)\n",
    "            fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 4))\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            for idx, d in enumerate(d_values):\n",
    "                ax = axes[idx]\n",
    "                if k in differences_dict[metric] and d in differences_dict[metric][k]:\n",
    "                    differences = differences_dict[metric][k][d]['differences']\n",
    "                    normality = differences_dict[metric][k][d]['normality']\n",
    "                    sns.histplot(differences, kde=True, ax=ax)\n",
    "                    ax.set_title(f'{metric} Differences (d={d}, k={k})\\nNormality: {\"Yes\" if normality else \"No\"}')\n",
    "                    ax.set_xlabel('Difference (Sorted - Unsorted)')\n",
    "                else:\n",
    "                    ax.axis('off')\n",
    "\n",
    "            # Remove any unused subplots\n",
    "            for idx in range(len(d_values), len(axes)):\n",
    "                axes[idx].axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, f'{metric}_differences_k{k}.png'))\n",
    "            plt.close()\n",
    "            print(f\"Saved histogram for {metric} at k={k} in {output_dir}\")\n",
    "\n",
    "# 6. Summarize Results\n",
    "def summarize_results(results, output_dir='statistical_results'):\n",
    "    \"\"\"\n",
    "    Save the statistical test results to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        results (list): List of dictionaries containing test results.\n",
    "        output_dir (str): Directory to save the results.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(os.path.join(output_dir, 'statistical_test_results.csv'), index=False)\n",
    "    print(f\"Saved statistical test results: {os.path.join(output_dir, 'statistical_test_results.csv')}\")\n",
    "\n",
    "# 7. Main Execution\n",
    "# Define the path to your CSV file\n",
    "summary_csv_path = 'results_temp/summary_evaluations.csv'\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "df = load_data(summary_csv_path)\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# Define the metrics to analyze\n",
    "metrics = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'Cosine Similarity']\n",
    "\n",
    "# Check if all metrics exist in the DataFrame\n",
    "missing_metrics = [metric for metric in metrics if metric not in df.columns]\n",
    "if missing_metrics:\n",
    "    raise ValueError(f\"The following metrics are missing in the data: {missing_metrics}\")\n",
    "\n",
    "# Get unique k and d values\n",
    "k_values = sorted(df['k'].unique())\n",
    "d_values = sorted(df['d'].unique())\n",
    "\n",
    "# Initialize list to store statistical test results\n",
    "test_results = []\n",
    "\n",
    "# Initialize list to store differences for DataFrame\n",
    "differences_records = []\n",
    "\n",
    "# Initialize nested dictionary to store differences for plotting\n",
    "differences_dict = {metric: {k: {} for k in k_values} for metric in metrics}\n",
    "\n",
    "# Iterate through each metric, k value, and d value\n",
    "for metric in metrics:\n",
    "    for k in k_values:\n",
    "        for d in d_values:\n",
    "            print(f\"\\nAnalyzing {metric} for k={k}, d={d}...\")\n",
    "            paired_df = prepare_paired_data(df, metric, d, k)\n",
    "\n",
    "            if paired_df.empty:\n",
    "                print(f\"No paired data available for {metric} with k={k}, d={d}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Perform statistical tests\n",
    "            test_result = perform_statistical_tests(paired_df)\n",
    "            test_result.update({\n",
    "                'Metric': metric,\n",
    "                'k': k,\n",
    "                'd': d\n",
    "            })\n",
    "            test_results.append(test_result)\n",
    "\n",
    "            # Store differences for plotting\n",
    "            differences = paired_df['sorted'] - paired_df['unsorted']\n",
    "            differences_dict[metric][k][d] = {\n",
    "                'differences': differences,\n",
    "                'shapiro': (test_result['Shapiro-Wilk Statistic'], test_result['Shapiro-Wilk p-value']),\n",
    "                'normality': test_result['Normality']\n",
    "            }\n",
    "\n",
    "            # Collect differences into records for DataFrame\n",
    "            for idx in paired_df.index:\n",
    "                differences_records.append({\n",
    "                    'Metric': metric,\n",
    "                    'k': k,\n",
    "                    'd': d,\n",
    "                    'sorted_score': paired_df.at[idx, 'sorted'],\n",
    "                    'unsorted_score': paired_df.at[idx, 'unsorted'],\n",
    "                    'difference': paired_df.at[idx, 'sorted'] - paired_df.at[idx, 'unsorted']\n",
    "                })\n",
    "\n",
    "# 8. Create and Inspect the Differences DataFrame\n",
    "# Create a DataFrame from the differences records\n",
    "differences_df = pd.DataFrame(differences_records)\n",
    "\n",
    "# Display the first few rows of the differences DataFrame\n",
    "print(\"\\nDifferences DataFrame:\")\n",
    "display(differences_df.head())\n",
    "\n",
    "# Optionally, display summary statistics\n",
    "print(\"\\nSummary Statistics of Differences:\")\n",
    "display(differences_df.describe())\n",
    "\n",
    "# 9. Summarize and Save Statistical Test Results\n",
    "summarize_results(test_results)\n",
    "\n",
    "# 10. Plot All Histograms in a Single Figure\n",
    "print(\"\\nCreating aggregated histograms...\")\n",
    "plot_all_histograms(differences_dict, metrics, d_values, k_values)\n",
    "print(\"Aggregated histograms created and saved successfully.\")\n",
    "\n",
    "# 11. Display Statistical Test Results\n",
    "# Create a DataFrame for statistical test results\n",
    "results_df = pd.DataFrame(test_results)\n",
    "\n",
    "# Display the statistical test results\n",
    "print(\"\\nStatistical Test Results:\")\n",
    "display(results_df)\n",
    "\n",
    "# 12. (Optional) Save the Differences DataFrame\n",
    "# Save the differences DataFrame to a CSV file for further inspection\n",
    "differences_df.to_csv('statistical_results/differences_dataframe.csv', index=False)\n",
    "print(\"Saved differences DataFrame: statistical_results/differences_dataframe.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set chatgpt 4o as the llmEval the environment variable for the key has already been set\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPEN_AI_API_KEY\n",
    "\n",
    "llmEval = OpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directory containing the CSV files\n",
    "DIRECTORY = 'results_temp'  # Ensure this path is correct\n",
    "\n",
    "# Regex pattern to parse filenames\n",
    "FILENAME_PATTERN = re.compile(\n",
    "    r'evaluated_(?P<sort_status>sorted|unsorted)_k(?P<k>\\d+)_d(?P<d>\\d+)\\.csv$'\n",
    ")\n",
    "\n",
    "# Function to parse filename and extract sort_status, k, d\n",
    "def parse_filename(filename):\n",
    "    \"\"\"\n",
    "    Parses the filename to extract sort status, k value, and d value.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The filename string.\n",
    "\n",
    "    Returns:\n",
    "        tuple or None: (sort_status, k_value, d_value) if pattern matches, else None\n",
    "    \"\"\"\n",
    "    match = FILENAME_PATTERN.match(filename)\n",
    "    if not match:\n",
    "        return None\n",
    "    sort_status = match.group('sort_status')\n",
    "    k_value = int(match.group('k'))\n",
    "    d_value = int(match.group('d'))\n",
    "    return sort_status, k_value, d_value\n",
    "\n",
    "# Function to append DataFrame to CSV with headers if file doesn't exist\n",
    "def append_df_to_csv(df, filepath):\n",
    "    \"\"\"\n",
    "    Appends a DataFrame to a CSV file. If the file does not exist, it creates it with headers.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to append.\n",
    "        filepath (str): Path to the CSV file.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(filepath):\n",
    "        df.to_csv(filepath, index=False)\n",
    "    else:\n",
    "        df.to_csv(filepath, mode='a', header=False, index=False)\n",
    "\n",
    "# Function to process a single pair of files\n",
    "def process_single_pair(sorted_file, unsorted_file):\n",
    "    \"\"\"\n",
    "    Processes a single pair of sorted and unsorted CSV files, evaluates unanswered questions,\n",
    "    and appends the results to the evaluation CSV.\n",
    "\n",
    "    Args:\n",
    "        sorted_file (str): Filename of the sorted CSV.\n",
    "        unsorted_file (str): Filename of the unsorted CSV.\n",
    "\n",
    "    Returns:\n",
    "        dict: Summary of evaluation counts for this pair.\n",
    "    \"\"\"\n",
    "    # Parse filenames to extract k and d\n",
    "    parsed_sorted = parse_filename(sorted_file)\n",
    "    parsed_unsorted = parse_filename(unsorted_file)\n",
    "\n",
    "    if not parsed_sorted or not parsed_unsorted:\n",
    "        print(\"Filename parsing failed. Ensure filenames match the expected pattern.\")\n",
    "        return None\n",
    "\n",
    "    _, k_sorted, d_sorted = parsed_sorted\n",
    "    _, k_unsorted, d_unsorted = parsed_unsorted\n",
    "\n",
    "    # Check if k and d match\n",
    "    if (k_sorted != k_unsorted) or (d_sorted != d_unsorted):\n",
    "        print(f\"k and d values do not match for files {sorted_file} and {unsorted_file}.\")\n",
    "        return None\n",
    "\n",
    "    k = k_sorted\n",
    "    d = d_sorted\n",
    "\n",
    "    print(f\"\\nProcessing files with k={k} and d={d}:\")\n",
    "    print(f\"Sorted File: {sorted_file}\")\n",
    "    print(f\"Unsorted File: {unsorted_file}\")\n",
    "\n",
    "    # Construct full file paths\n",
    "    sorted_filepath = os.path.join(DIRECTORY, sorted_file)\n",
    "    unsorted_filepath = os.path.join(DIRECTORY, unsorted_file)\n",
    "\n",
    "    # Load the sorted and unsorted CSV files\n",
    "    try:\n",
    "        sorted_data = pd.read_csv(sorted_filepath)\n",
    "        unsorted_data = pd.read_csv(unsorted_filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading files: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Ensure all entries in 'Question', 'Answer', and 'Model Answer' are strings\n",
    "    for df, name in zip([sorted_data, unsorted_data], ['sorted', 'unsorted']):\n",
    "        for col in ['Question', 'Answer', 'Model Answer']:\n",
    "            if col not in df.columns:\n",
    "                print(f\"Column '{col}' not found in {name} file.\")\n",
    "                return None\n",
    "            df[col] = df[col].astype(str)\n",
    "\n",
    "    # Rename 'Model Answer' columns to differentiate between sorted and unsorted\n",
    "    sorted_data.rename(columns={'Model Answer': 'Model Answer_sorted'}, inplace=True)\n",
    "    unsorted_data.rename(columns={'Model Answer': 'Model Answer_unsorted'}, inplace=True)\n",
    "\n",
    "    # Merge sorted and unsorted data on 'Question' and 'Answer' to ensure alignment\n",
    "    merged_data = pd.merge(\n",
    "        sorted_data,\n",
    "        unsorted_data,\n",
    "        on=['Question', 'Answer'],\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    # If there are no matching questions, return\n",
    "    if merged_data.empty:\n",
    "        print(f\"No matching questions found between sorted and unsorted files for k={k}, d={d}.\")\n",
    "        return None\n",
    "\n",
    "    # Add 'k' and 'd' columns\n",
    "    merged_data['k'] = k\n",
    "    merged_data['d'] = d\n",
    "\n",
    "    # Define the per-pair evaluation CSV filepath\n",
    "    pair_evaluation_filename = f'evaluation_k{k}_d{d}.csv'\n",
    "    pair_evaluation_filepath = os.path.join(DIRECTORY, pair_evaluation_filename)\n",
    "\n",
    "    # Load existing evaluations for this pair to identify already processed questions\n",
    "    if os.path.isfile(pair_evaluation_filepath):\n",
    "        existing_evaluations = pd.read_csv(pair_evaluation_filepath)\n",
    "        processed_questions = set(existing_evaluations['Question'].tolist())\n",
    "        print(f\"Found {len(processed_questions)} already evaluated questions for k={k}, d={d}.\")\n",
    "    else:\n",
    "        existing_evaluations = pd.DataFrame(columns=[\n",
    "            'Question', 'Answer', 'Model Answer_sorted', 'Model Answer_unsorted',\n",
    "            'Preferred Model', 'Sorted Correctness', 'Unsorted Correctness', 'k', 'd'\n",
    "        ])\n",
    "        processed_questions = set()\n",
    "        print(f\"No existing evaluations found for k={k}, d={d}. Processing all questions.\")\n",
    "\n",
    "    # Identify questions that have not been evaluated yet\n",
    "    to_evaluate = merged_data[~merged_data['Question'].isin(processed_questions)]\n",
    "\n",
    "    print(f\"Number of questions to evaluate: {to_evaluate.shape[0]}\")\n",
    "\n",
    "    if to_evaluate.empty:\n",
    "        print(f\"No new questions to evaluate for k={k}, d={d}.\")\n",
    "        return None\n",
    "\n",
    "    # Initialize list to store evaluation results\n",
    "    evaluation_results = []\n",
    "\n",
    "    # Iterate through each row with a progress bar\n",
    "    for idx, row in tqdm(to_evaluate.iterrows(), total=to_evaluate.shape[0], desc=\"Evaluating answers\", leave=False):\n",
    "        question = row['Question']\n",
    "        correct_answer = row['Answer']\n",
    "        sorted_answer = row['Model Answer_sorted']\n",
    "        unsorted_answer = row['Model Answer_unsorted']\n",
    "\n",
    "       \n",
    "        prompt = f\"\"\"\n",
    "The question is: {question}\n",
    "The correct answer is: {correct_answer}\n",
    "The sorted model's answer is: {sorted_answer}\n",
    "The unsorted model's answer is: {unsorted_answer}\n",
    "\n",
    "Instructions:\n",
    "1. For both the sorted and unsorted model answers, assess each as one of the following: correct, partially correct, wrong, or did not find.\n",
    "2. Based on the assessments, determine which model's answer is closer to the correct answer:\n",
    "   - If one is correct and the other is partially correct or wrong, the correct one should be preferred.\n",
    "   - If one is partially correct and the other is wrong, the partially correct one should be preferred.\n",
    "   - If both are correct, decide which is preferred based on completeness, accuracy, or any additional information relative to the correct answer.\n",
    "   - If both are wrong, \"neither\" should be preferred.\n",
    "   - If both are identical in correctness (e.g., both correct, both partially correct, or both wrong with no preference), the result should be \"tie.\"\n",
    "\n",
    "Respond in the following format:\n",
    "<preferred model>|<sorted correctness>|<unsorted correctness>\n",
    "\n",
    "Definitions:\n",
    "- <preferred model>: Choose one of the following: sorted, unsorted, tie, neither\n",
    "- <sorted correctness> and <unsorted correctness>: Choose one of the following for each: correct, partially correct, wrong, did not find\n",
    "\n",
    "Note:\n",
    "- Ensure that the more correct answer (or the more complete/relevant answer) is always preferred, even if both are marked as correct.\n",
    "- Provide only the specified response format with no additional text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            # Call your existing language model's evaluation method\n",
    "            response = llmEval.complete(prompt)\n",
    "            best_answer = response.text.strip().lower()  # Normalize the response\n",
    "\n",
    "            # Split the response into components\n",
    "            parts = best_answer.split('|')\n",
    "            if len(parts) != 3:\n",
    "                print(f\"Invalid response format for question '{question}'.\")\n",
    "                preferred_model = 'invalid_response'\n",
    "                sorted_correctness = 'invalid_response'\n",
    "                unsorted_correctness = 'invalid_response'\n",
    "            else:\n",
    "                preferred_model = parts[0].strip()\n",
    "                sorted_correctness = parts[1].strip()\n",
    "                unsorted_correctness = parts[2].strip()\n",
    "\n",
    "                valid_models = ['sorted', 'unsorted', 'tie', 'neither']\n",
    "                valid_correctness = ['correct', 'partially correct', 'wrong', 'did not find']\n",
    "\n",
    "                if preferred_model not in valid_models:\n",
    "                    preferred_model = 'invalid_response'\n",
    "                if sorted_correctness not in valid_correctness:\n",
    "                    sorted_correctness = 'invalid_response'\n",
    "                if unsorted_correctness not in valid_correctness:\n",
    "                    unsorted_correctness = 'invalid_response'\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating question '{question}': {e}\")\n",
    "            preferred_model = 'invalid_response'\n",
    "            sorted_correctness = 'invalid_response'\n",
    "            unsorted_correctness = 'invalid_response'\n",
    "\n",
    "        # Append the result to the evaluation_results list\n",
    "        evaluation_results.append({\n",
    "            'Question': question,\n",
    "            'Answer': correct_answer,\n",
    "            'Model Answer_sorted': sorted_answer,\n",
    "            'Model Answer_unsorted': unsorted_answer,\n",
    "            'Preferred Model': preferred_model,\n",
    "            'Sorted Correctness': sorted_correctness,\n",
    "            'Unsorted Correctness': unsorted_correctness,\n",
    "            'k': k,\n",
    "            'd': d\n",
    "        })\n",
    "\n",
    "        # Save the result immediately to the CSV file\n",
    "        evaluation_df = pd.DataFrame([evaluation_results[-1]])\n",
    "        append_df_to_csv(evaluation_df, pair_evaluation_filepath)\n",
    "\n",
    "    # Generate summary for this pair\n",
    "    existing_evaluations = pd.read_csv(pair_evaluation_filepath)\n",
    "\n",
    "    # Count preferred models\n",
    "    preferred_counts = existing_evaluations['Preferred Model'].value_counts().to_dict()\n",
    "\n",
    "    # Count correctness for sorted and unsorted answers\n",
    "    sorted_correctness_counts = existing_evaluations['Sorted Correctness'].value_counts().to_dict()\n",
    "    unsorted_correctness_counts = existing_evaluations['Unsorted Correctness'].value_counts().to_dict()\n",
    "\n",
    "    summary_complete = {\n",
    "        'k': k,\n",
    "        'd': d,\n",
    "        'Preferred Sorted': preferred_counts.get('sorted', 0),\n",
    "        'Preferred Unsorted': preferred_counts.get('unsorted', 0),\n",
    "        'Preferred Tie': preferred_counts.get('tie', 0),\n",
    "        'Preferred Neither': preferred_counts.get('neither', 0),\n",
    "        'Preferred Invalid': preferred_counts.get('invalid_response', 0),\n",
    "        'Sorted Correct': sorted_correctness_counts.get('correct', 0),\n",
    "        'Sorted Partially Correct': sorted_correctness_counts.get('partially correct', 0),\n",
    "        'Sorted Wrong': sorted_correctness_counts.get('wrong', 0),\n",
    "        'Sorted Did Not Find': sorted_correctness_counts.get('did not find', 0),\n",
    "        'Sorted Invalid': sorted_correctness_counts.get('invalid_response', 0),\n",
    "        'Unsorted Correct': unsorted_correctness_counts.get('correct', 0),\n",
    "        'Unsorted Partially Correct': unsorted_correctness_counts.get('partially correct', 0),\n",
    "        'Unsorted Wrong': unsorted_correctness_counts.get('wrong', 0),\n",
    "        'Unsorted Did Not Find': unsorted_correctness_counts.get('did not find', 0),\n",
    "        'Unsorted Invalid': unsorted_correctness_counts.get('invalid_response', 0)\n",
    "    }\n",
    "\n",
    "    return summary_complete\n",
    "\n",
    "# Now, process all file pairs\n",
    "\n",
    "# Find all sorted and unsorted files and group them by k and d\n",
    "file_groups = {}\n",
    "for filename in os.listdir(DIRECTORY):\n",
    "    if not filename.endswith(\".csv\"):\n",
    "        continue\n",
    "    parsed = parse_filename(filename)\n",
    "    if not parsed:\n",
    "        continue\n",
    "    sort_status, k, d = parsed\n",
    "    key = (k, d)\n",
    "    if key not in file_groups:\n",
    "        file_groups[key] = {}\n",
    "    file_groups[key][sort_status] = filename\n",
    "\n",
    "# Initialize list to collect summaries\n",
    "summary_records = []\n",
    "\n",
    "# Iterate through each group of sorted and unsorted files\n",
    "for (k, d), files in tqdm(file_groups.items(), desc=\"Processing file groups\"):\n",
    "    sorted_filename = files.get('sorted')\n",
    "    unsorted_filename = files.get('unsorted')\n",
    "\n",
    "    if not sorted_filename or not unsorted_filename:\n",
    "        print(f\"Missing sorted or unsorted file for k={k}, d={d}. Skipping this group.\")\n",
    "        continue\n",
    "\n",
    "    # Process the single pair and get the summary\n",
    "    summary = process_single_pair(sorted_filename, unsorted_filename)\n",
    "\n",
    "    if summary:\n",
    "        # Append the summary to the summary_records list\n",
    "        summary_records.append(summary)\n",
    "\n",
    "        # Optionally, print the summary for this pair\n",
    "        print(f\"\\nSummary for k={k}, d={d}:\")\n",
    "        print(f\"  Preferred Model Counts:\")\n",
    "        print(f\"    Sorted: {summary.get('Preferred Sorted', 0)}\")\n",
    "        print(f\"    Unsorted: {summary.get('Preferred Unsorted', 0)}\")\n",
    "        print(f\"    Tie: {summary.get('Preferred Tie', 0)}\")\n",
    "        print(f\"    Neither: {summary.get('Preferred Neither', 0)}\")\n",
    "        print(f\"    Invalid Response: {summary.get('Preferred Invalid', 0)}\")\n",
    "\n",
    "        print(f\"  Sorted Correctness Counts:\")\n",
    "        print(f\"    Correct: {summary.get('Sorted Correct', 0)}\")\n",
    "        print(f\"    Partially Correct: {summary.get('Sorted Partially Correct', 0)}\")\n",
    "        print(f\"    Wrong: {summary.get('Sorted Wrong', 0)}\")\n",
    "        print(f\"    Did Not Find: {summary.get('Sorted Did Not Find', 0)}\")\n",
    "        print(f\"    Invalid Response: {summary.get('Sorted Invalid', 0)}\")\n",
    "\n",
    "        print(f\"  Unsorted Correctness Counts:\")\n",
    "        print(f\"    Correct: {summary.get('Unsorted Correct', 0)}\")\n",
    "        print(f\"    Partially Correct: {summary.get('Unsorted Partially Correct', 0)}\")\n",
    "        print(f\"    Wrong: {summary.get('Unsorted Wrong', 0)}\")\n",
    "        print(f\"    Did Not Find: {summary.get('Unsorted Did Not Find', 0)}\")\n",
    "        print(f\"    Invalid Response: {summary.get('Unsorted Invalid', 0)}\")\n",
    "    else:\n",
    "        print(f\"No new evaluations for k={k}, d={d}.\")\n",
    "\n",
    "# After processing all pairs, generate the summary DataFrame\n",
    "if summary_records:\n",
    "    summary_df = pd.DataFrame(summary_records)\n",
    "    # Define the summary CSV filepath\n",
    "    summary_csv_path = os.path.join(DIRECTORY, 'summary_evaluations.csv')\n",
    "\n",
    "    # Save the summary DataFrame to a CSV file\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"\\nSummary evaluation results saved to {summary_csv_path}\")\n",
    "\n",
    "    # Display the summary DataFrame\n",
    "    print(\"\\nSummary Evaluation Results for All Pairs:\")\n",
    "    display(summary_df)\n",
    "else:\n",
    "    print(\"No evaluation records to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No evaluation files found.\n",
      "No data to display.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the directory containing the evaluation CSV files\n",
    "DIRECTORY = 'results_temp'  # Ensure this path is correct\n",
    "\n",
    "# Regex pattern to identify evaluation files\n",
    "EVALUATION_FILENAME_PATTERN = re.compile(\n",
    "    r'evaluation_k(?P<k>\\d+)_d(?P<d>\\d+)\\.csv$'\n",
    ")\n",
    "\n",
    "# Initialize an empty list to collect DataFrames\n",
    "evaluation_dfs = []\n",
    "\n",
    "# Iterate over files in the directory and read evaluation CSV files\n",
    "for filename in os.listdir(DIRECTORY):\n",
    "    match = EVALUATION_FILENAME_PATTERN.match(filename)\n",
    "    if match:\n",
    "        k = int(match.group('k'))\n",
    "        d = int(match.group('d'))\n",
    "        filepath = os.path.join(DIRECTORY, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        df['k'] = k\n",
    "        df['d'] = d\n",
    "        evaluation_dfs.append(df)\n",
    "\n",
    "# Concatenate all evaluation DataFrames into one\n",
    "if evaluation_dfs:\n",
    "    evaluation_data = pd.concat(evaluation_dfs, ignore_index=True)\n",
    "else:\n",
    "    print(\"No evaluation files found.\")\n",
    "    evaluation_data = pd.DataFrame()\n",
    "\n",
    "# Check if data is loaded\n",
    "if not evaluation_data.empty:\n",
    "    # Display the first few rows of the data\n",
    "    display(evaluation_data.head())\n",
    "    \n",
    "    # Correctness categories (excluding 'invalid_response')\n",
    "    correctness_categories = ['correct', 'partially correct', 'wrong', 'did not find']\n",
    "    \n",
    "    # Preferred model categories\n",
    "    preferred_model_categories = ['sorted', 'unsorted', 'tie', 'neither']\n",
    "    \n",
    "    # Set up improved plotting style with lighter colors\n",
    "    sns.set_theme(style='whitegrid', palette='pastel', context='talk')\n",
    "    plt.rcParams.update({'figure.figsize': (14, 8), 'font.size': 14})\n",
    "    \n",
    "    # Function to add count labels on top of bars\n",
    "    def add_counts(ax):\n",
    "        for container in ax.containers:\n",
    "            ax.bar_label(container, fmt='%d', label_type='edge', fontsize=12, padding=3)\n",
    "    \n",
    "    # Prepare data for correctness plots\n",
    "    correctness_data = evaluation_data.melt(\n",
    "        id_vars=['k', 'd'],\n",
    "        value_vars=['Sorted Correctness', 'Unsorted Correctness'],\n",
    "        var_name='Model',\n",
    "        value_name='Correctness'\n",
    "    )\n",
    "    correctness_data['Model'] = correctness_data['Model'].str.replace(' Correctness', '')\n",
    "    \n",
    "    # Exclude 'invalid_response' from correctness data\n",
    "    correctness_data = correctness_data[correctness_data['Correctness'] != 'invalid_response']\n",
    "    \n",
    "    # Prepare data for preferred model plots\n",
    "    preferred_data = evaluation_data[['k', 'd', 'Preferred Model', 'Sorted Correctness', 'Unsorted Correctness']]\n",
    "    \n",
    "    # Iterate over each unique value of k\n",
    "    for k_value in sorted(evaluation_data['k'].unique()):\n",
    "        print(f\"\\nGenerating correctness plots for k={k_value}\")\n",
    "        \n",
    "        # Filter data for the current k value\n",
    "        k_correctness_data = correctness_data[correctness_data['k'] == k_value]\n",
    "        \n",
    "        # Prepare counts\n",
    "        counts = k_correctness_data.groupby(['d', 'Correctness', 'Model']).size().reset_index(name='Count')\n",
    "        counts.sort_values('d', inplace=True)  # Ensure d is sorted in ascending order\n",
    "        \n",
    "        # Define custom color palette with lighter shades for models\n",
    "        model_palette = {\n",
    "            'Sorted': '#99d6ff',    # Light blue\n",
    "            'Unsorted': '#ffcc99'   # Light orange\n",
    "        }\n",
    "        \n",
    "        # Specify the order of correctness categories\n",
    "        correctness_order = ['correct', 'partially correct', 'wrong', 'did not find']\n",
    "        \n",
    "        # Create the barplot using FacetGrid with Correctness as columns\n",
    "        # Adjust the legend position slightly to the left\n",
    "        g = sns.catplot(\n",
    "        data=counts,\n",
    "        x='d',\n",
    "        y='Count',\n",
    "        hue='Model',\n",
    "        col='Correctness',\n",
    "        kind='bar',\n",
    "        palette=model_palette,\n",
    "        height=6,\n",
    "        aspect=1.2,\n",
    "        legend_out=True,  # Keep the legend out of the grid\n",
    "        col_order=correctness_order,\n",
    "        col_wrap=2  # Arrange subplots in a 2x2 grid\n",
    "    )\n",
    "\n",
    "        # Set the main title and labels\n",
    "        g.fig.subplots_adjust(top=0.85, right=0.88)  # Adjust right margin to give space to the legend\n",
    "        g.fig.suptitle(f'Correctness Counts by Model and Depth (k={k_value})', fontsize=16)\n",
    "        g.set_axis_labels('Depth', 'Count')\n",
    "\n",
    "        # Position legend slightly left within the adjusted space\n",
    "        g._legend.set_bbox_to_anchor((1.02, 0.5))  # Slightly move it left from 1.05 to 1.02\n",
    "        g._legend.set_title('Model')\n",
    "        g._legend.set_frame_on(True)\n",
    "            \n",
    "        # Add count labels\n",
    "        for ax in g.axes.flat:\n",
    "            add_counts(ax)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nGenerating preferred model plots for k={k_value}\")\n",
    "        \n",
    "        # Filter data for the current k value\n",
    "        k_preferred_data = preferred_data[preferred_data['k'] == k_value]\n",
    "        \n",
    "        # Exclude instances where the preferred model is 'sorted' or 'unsorted' and both correctness are 'wrong' or 'did not find'\n",
    "        condition = ~(\n",
    "            (k_preferred_data['Preferred Model'].isin(['sorted', 'unsorted'])) &\n",
    "            (k_preferred_data['Sorted Correctness'].isin(['wrong', 'did not find'])) &\n",
    "            (k_preferred_data['Unsorted Correctness'].isin(['wrong', 'did not find']))\n",
    "        )\n",
    "        k_preferred_data = k_preferred_data[condition]\n",
    "        \n",
    "        # Filter to only 'sorted' and 'unsorted' preferred models\n",
    "        model_preference = k_preferred_data[k_preferred_data['Preferred Model'].isin(['sorted', 'unsorted'])]\n",
    "        \n",
    "        # Prepare counts\n",
    "        counts = model_preference.groupby(['d', 'Preferred Model']).size().reset_index(name='Count')\n",
    "        counts.sort_values('d', inplace=True)  # Ensure d is sorted in ascending order\n",
    "        \n",
    "        # Define custom color palette for preferred models\n",
    "        preferred_palette = {\n",
    "            'sorted': '#99d6ff',    # Light blue\n",
    "            'unsorted': '#ffcc99'   # Light orange\n",
    "        }\n",
    "        \n",
    "        # Initialize the matplotlib figure\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Create the barplot\n",
    "        ax = sns.barplot(\n",
    "            x='d',\n",
    "            y='Count',\n",
    "            hue='Preferred Model',\n",
    "            data=counts,\n",
    "            palette=preferred_palette\n",
    "        )\n",
    "        \n",
    "        # Add title and labels with improved formatting\n",
    "        plt.title(f'Preferred Model Counts by d (k={k_value})', fontsize=16)\n",
    "        plt.xlabel('d (Dimension)', fontsize=14)\n",
    "        plt.ylabel('Count', fontsize=14)\n",
    "        \n",
    "        # Add count labels on top of each bar\n",
    "        add_counts(ax)\n",
    "        \n",
    "        # Enhance legend\n",
    "        plt.legend(title='Preferred Model', fontsize=12, title_fontsize=14)\n",
    "        \n",
    "        # Improve layout\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No data to display.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nzta_experiment_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
