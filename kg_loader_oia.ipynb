{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: this wont run properly if run with llama_index versions lower than 11 (becaus of triplet parser function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook fills the knowledge graph with the OIA experiment files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipenavarro/Documents/Auckland/nzta/nzta_experiment/NZTA-GraphRAG-Experiment/nzta_experiment_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "from llama_index.core import Settings, ServiceContext, StorageContext, SimpleDirectoryReader\n",
    "from llama_index.llms.groq import Groq as Groq_llamaindex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.graph_stores.neo4j import Neo4jGraphStore, Neo4jPropertyGraphStore\n",
    "from milvus import default_server\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor, SimpleLLMPathExtractor, ImplicitPathExtractor, DynamicLLMPathExtractor\n",
    "\n",
    "from llama_index.core.node_parser import (\n",
    "    SentenceSplitter,\n",
    "    SemanticSplitterNodeParser,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Retrieve API keys and credentials securely\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "OPEN_AI_API_KEY = os.getenv('OPEN_AI_API_KEY')\n",
    "REPLICATED_API_KEY = os.getenv('REPLICATED_API_KEY')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_URL = os.getenv('NEO4J_URL', 'bolt://localhost:7687')\n",
    "NEO4J_DATABASE = os.getenv('NEO4J_DATABASE', 'neo4j')\n",
    "\n",
    "\n",
    "client = Groq(api_key = GROQ_API_KEY)\n",
    "\n",
    "llm = Groq_llamaindex(model=\"llama3-groq-70b-8192-tool-use-preview\",\n",
    "                       api_key=GROQ_API_KEY, #using data uploader for now\n",
    "                       temperature=0)\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "StorageContext.llm = llm\n",
    "ServiceContext.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = NEO4J_USERNAME\n",
    "password =  NEO4J_PASSWORD\n",
    "url = NEO4J_URL\n",
    "database = NEO4J_DATABASE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "property_graph_store = Neo4jPropertyGraphStore(\n",
    "    username=username,\n",
    "    password=password,\n",
    "    url=url,\n",
    "    database=database,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(property_graph_store=property_graph_store)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following text block is in charge of the upload. It keeps track of the already uploaded files so it can be run multiple times and it will continue where it left off. If you want to run all files from scratch you need to delete the file processed_files.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed files are: set()\n",
      "Processing batch 1/50: ['OIA-10601-response-georgina-campbell.pdf', 'oia-12662-response-letter.pdf']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 5/5 [00:00<00:00, 1779.21it/s]\n",
      "Extracting and inferring knowledge graph from text: 100%|██████████| 8/8 [00:24<00:00,  3.03s/it]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "Generating embeddings: 100%|██████████| 39/39 [00:05<00:00,  7.03it/s]\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded processed files in processed_files.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 145\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecorded processed files in processed_files.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Wait for the specified number of minutes\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_minutes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Delete the processed files from the target directory\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m current_batch:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor, SimpleLLMPathExtractor, ImplicitPathExtractor\n",
    "from llama_index.core import KnowledgeGraphIndex, SimpleDirectoryReader\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "# Initialize directories, batch size, wait time, and skip count\n",
    "\n",
    "source_dir = \"transport_data/nzta/simple_files_set\"\n",
    "target_dir = \"transport_data//nzta/temp_files\"\n",
    "batch_size = 2\n",
    "wait_minutes = 1  # Specify the wait time in minutes\n",
    "skip_files = 0  # Number of files to skip\n",
    "\n",
    "# Create the target directory if it does not exist\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "    \n",
    "\n",
    "# Start by deleting anything under temp_files but not the directory itself\n",
    "if os.path.exists(target_dir):\n",
    "    for file_name in os.listdir(target_dir):\n",
    "        file_path = os.path.join(target_dir, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        else:\n",
    "            shutil.rmtree(file_path)\n",
    "\n",
    "# Read the list of already processed files\n",
    "processed_files = set()\n",
    "if os.path.exists(\"processed_files.txt\"):\n",
    "    print(\"Processed files exist\")\n",
    "    with open(\"processed_files.txt\", \"r\") as f:\n",
    "        processed_files = set(line.strip() for line in f)\n",
    "\n",
    "# List all files in the source directory\n",
    "full_file_list = os.listdir(source_dir)\n",
    "print(f\"Processed files are: {processed_files}\")\n",
    "# Filter out files that have already been processed\n",
    "full_file_list = [file for file in full_file_list if file not in processed_files]\n",
    "\n",
    "# Optionally, skip the first 'k' files\n",
    "if skip_files > 0:\n",
    "    full_file_list = full_file_list[skip_files:]\n",
    "\n",
    "# Calculate the number of batches needed\n",
    "num_batches = len(full_file_list) // batch_size + (1 if len(full_file_list) % batch_size > 0 else 0)\n",
    "\n",
    "for i in range(num_batches):\n",
    "    # Determine the start and end indices for the current batch\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min(start_idx + batch_size, len(full_file_list))\n",
    "\n",
    "    # Get the current batch of files\n",
    "    current_batch = full_file_list[start_idx:end_idx]\n",
    "    print(f\"Processing batch {i+1}/{num_batches}: {current_batch}\")\n",
    "\n",
    "    # Copy the current batch of files to the target directory\n",
    "    for file_name in current_batch:\n",
    "        shutil.copy(os.path.join(source_dir, file_name), os.path.join(target_dir, file_name))\n",
    "\n",
    "    # Process the current batch of files\n",
    "    graph_documents = SimpleDirectoryReader(target_dir).load_data()\n",
    "\n",
    "    Settings.chunk_size = 512\n",
    "    Settings.chunk_overlap = 20\n",
    "\n",
    "    extract_prompt = (\n",
    "    \"Extract up to {max_knowledge_triplets} knowledge triplets from the given text. \"\n",
    "    \"Each triplet should be in the form of (head, relation, tail) with their respective types and properties.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"INITIAL ONTOLOGY:\\n\"\n",
    "    \"Entity Types: {allowed_entity_types}\\n\"\n",
    "    \"Entity Properties: {allowed_entity_properties}\\n\"\n",
    "    \"Relation Types: {allowed_relation_types}\\n\"\n",
    "    \"Relation Properties: {allowed_relation_properties}\\n\"\n",
    "    \"\\n\"\n",
    "    \"Use these types as a starting point, but introduce new types if necessary based on the context.\\n\"\n",
    "    \"\\n\"\n",
    "    \"GUIDELINES:\\n\"\n",
    "    \"- Output in JSON format: [{{'head': '', 'head_type': '', 'head_props': {{...}}, 'relation': '', 'relation_props': {{...}}, 'tail': '', 'tail_type': '', 'tail_props': {{...}}}}]\\n\"\n",
    "    \"- Use the most complete form for entities (e.g., 'New Zealand Transport Agency' instead of abbreviations)\\n\"\n",
    "    \"For any reference to the New Zealand Transport Agency, including variations such as 'NZ Transport Agency Waka Kotahi', ‘NZ Transport Agency,’ ‘National Transport Agency,’ ‘Waka Kotahi,’ or any similar phrasing, create the entity node as 'New Zealand Transport Agency.' Ensure all variations of this entity are consistently mapped to 'New Zealand Transport Agency' in the output.\\n\"\n",
    "    \"- Do not create entities like 'request' or 'response' as they appear frequently and do not contribute to meaningful graph structure.\\n\"\n",
    "    \"- Do not create entities that are numbers, prices, or non-essential terms (e.g., dates unless critical to the meaning).\\n\"\n",
    "    \"- Keep entities concise (3-5 words max).\\n\"\n",
    "    \"- Break down complex phrases into multiple triplets for better granularity.\\n\"\n",
    "    \"- Ensure the knowledge graph is coherent and easily understandable.\\n\"\n",
    "    \"- Ensure that all property values are primitive types (e.g., String, Integer, Float, Boolean) or arrays of these types. Do not use maps or other complex structures.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"EXAMPLE:\\n\"\n",
    "    \"Text: The New Zealand Transport Agency approved the funding for road maintenance on State Highway 1.\\n\"\n",
    "    \"Wellington City Council is collaborating with the New Zealand Transport Agency on improving road safety initiatives.\\n\"\n",
    "    \"Output:\\n\"\n",
    "    \"[{{'head': 'New Zealand Transport Agency', 'head_type': 'AGENCY', 'head_props': {{'prop1': 'val', ...}}, \"\n",
    "    \"'relation': 'APPROVES', 'relation_props': {{'prop1': 'val', ...}}, \"\n",
    "    \"'tail': 'funding for road maintenance', 'tail_type': 'FUNDING_DECISION', 'tail_props': {{'prop1': 'val', ...}}}},\\n\"\n",
    "    \" {{'head': 'Wellington City Council', 'head_type': 'ORGANIZATION', 'head_props': {{'prop1': 'val', ...}}, \"\n",
    "    \"'relation': 'COLLABORATES_WITH', 'relation_props': {{'prop1': 'val', ...}}, \"\n",
    "    \"'tail': 'New Zealand Transport Agency', 'tail_type': 'AGENCY', 'tail_props': {{'prop1': 'val', ...}}}},\\n\"\n",
    "    \" {{'head': 'road safety initiatives', 'head_type': 'PROJECT', 'head_props': {{'prop1': 'val', ...}}, \"\n",
    "    \"'relation': 'IMPROVES', 'relation_props': {{'prop1': 'val', ...}}, \"\n",
    "    \"'tail': 'State Highway 1', 'tail_type': 'INFRASTRUCTURE', 'tail_props': {{'prop1': 'val', ...}}}}]\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Text: {text}\\n\"\n",
    "    \"Output:\\n\"\n",
    ")\n",
    "    \n",
    "    kg_extractor = DynamicLLMPathExtractor(\n",
    "    llm=llm,\n",
    "    extract_prompt=extract_prompt,\n",
    "    max_triplets_per_chunk=15,\n",
    "    \n",
    "    num_workers=4,\n",
    "    # Let the LLM infer entities and their labels (types) on the fly\n",
    "    allowed_entity_types=['People', 'Organizations', 'Locations', 'Processes', 'Agreements', 'Buildings', 'Streets', 'Projects', 'Events','Policies', 'Rules', 'Laws'],\n",
    "    # Let the LLM infer relationships on the fly\n",
    "    allowed_relation_types=None,\n",
    "    # LLM will generate any entity properties, set `None` to skip property generation (will be faster without)\n",
    "    allowed_relation_props=[],\n",
    "    # LLM will generate any relation properties, set `None` to skip property generation (will be faster without)\n",
    "    allowed_entity_props=[],\n",
    "    )\n",
    "\n",
    "    graph_index = PropertyGraphIndex.from_documents(\n",
    "    graph_documents,\n",
    "    property_graph_store=property_graph_store,\n",
    "    storage_context=storage_context,\n",
    "    kg_extractors=[kg_extractor],\n",
    "    embed_kg_nodes=True,\n",
    "    show_progress=True)\n",
    "\n",
    "    # Record the processed files immediately after processing\n",
    "    with open(\"processed_files.txt\", \"a\") as f:\n",
    "        for file_name in current_batch:\n",
    "            f.write(f\"{file_name}\\n\")\n",
    "            f.flush()  # Ensure data is written to disk\n",
    "    print('Recorded processed files in processed_files.txt')\n",
    "\n",
    "    # Wait for the specified number of minutes\n",
    "    time.sleep(wait_minutes * 60)\n",
    "\n",
    "    # Delete the processed files from the target directory\n",
    "    for file_name in current_batch:\n",
    "        os.remove(os.path.join(target_dir, file_name))\n",
    "\n",
    "    print(f\"Batch {i+1}/{num_batches} processed and cleaned up.\\n\")\n",
    "\n",
    "print(\"All files processed. Processed files list updated in 'processed_files.txt'.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nzta_experiment_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
